#!/usr/bin/env python2

import sys, argparse, distutils, os, logging
from distutils.spawn import find_executable
from snpiphy import configure_outdir, SnpiPhy, bad_options_check

# Import arguments
parser = argparse.ArgumentParser(description="snpiphy - An automated snp phylogeny pipeline.")
optional = parser._action_groups.pop()
required = parser.add_argument_group('required arguments')
required_input = parser.add_argument_group('input arguments - required, provide only one')


required.add_argument("-o", "--output-directory", required=True, help="Path to the output directory. A directory will be created if one does not exist.", action="store", dest="outdir")
required.add_argument("-r", "--reference", required=True, help="Path to the reference sequence. Only fasta format is accepted currently.", action="store", dest="reference")
required_input.add_argument("-q", "--input_dir", help="Path to a directory with your interleaved fastq sequencing reads, single-end sequencing reads (use with the -s option) or fasta assemblies.", action="store", dest="reads_dir", default=False)
required_input.add_argument("-l", "--input_list", help="Path to a tab-separated file containing read paths and paired status. Best used when running a combination of single-end and paired-end data or if your data is spread across multiple directories. Run 'snpiphy_generate_input_list' to generate an example list.", action="store", dest="reads_list", default=False)
optional.add_argument("-c", "--cov_cutoff", help="Percent coverage of reference sequence (0-100%%) used to reject a sample. Samples lower than this threshold will be excluded from phylogenetic pipeline steps. Defaults to 85%%.", action="store", dest="cutoff", type=int, default=85)
optional.add_argument('-p', '--prefix', help='Prefix for output files', action='store', dest='prefix', default='snpiphy_run')
optional.add_argument("-t", "--threads", help="Number of threads to use for multiprocessing.", action="store", dest="threads", type=int, default=1)
optional.add_argument("-j", "--parallel", help="Use GNU parallel to run multiple instances of snippy (can speed things up if you have multiple cores available)", action='store_true', default=False)
optional.add_argument("-s", "--single_end", help="fastq reads are single end instead of paired-end. Use for ion torrent or non-paired end illumina data", action='store_true', default=False)
optional.add_argument("-m", "--gamma_model", help="Use GTRGAMMA model instead of GTRCAT during the gubbins and RAxML tree building steps.", action='store_true', default=False)
optional.add_argument("-b", "--tree_builder", help="Algorithm used for building the phylogenetic tree (default: raxml)", action='store', choices=['raxml','fasttree'], default='raxml')
optional.add_argument("-f", "--force", help="Overwrite files in the output directories.", action='store_true', default=False)
optional.add_argument("-n", "--no_recombination_filter", help="Don't filter potential recombination events. Use for organisms that are known undergo low amounts of recombination.", action='store_true', default=False)
optional.add_argument('--version', action='version', version='%(prog)s 0.4')
optional.add_argument('-v', '--verbose', help="Increase verbosity on command line output (n.b. verbose output is always saved to snpiphy.log in the output directory).", action='store_true', default=False)

parser._action_groups.append(optional)

args = parser.parse_args()

if args.reads_dir == False and args.reads_list == False:
	print("Argument ERROR: No input arguments given. Please provide one input argument. Run snpiphy -h for details.")
	sys.exit(1)
elif args.reads_dir != False and args.reads_list != False:
	print("Argument ERROR: Multiple input arguments given. Please provide one input argument. Run snpiphy -h for details.")
	sys.exit(1)


configure_outdir(os.path.abspath(args.outdir), args.force)

logger = logging.getLogger("snpiphy")
logger.setLevel(logging.DEBUG)
fh = logging.FileHandler(os.path.join(args.outdir, '{}.log'.format('.'.join((args.prefix, 'snpiphy')))))
fh.setLevel(logging.DEBUG)
ch = logging.StreamHandler()
if args.verbose == True:
	ch.setLevel(logging.DEBUG)
	formatter = logging.Formatter('%(asctime)s:%(name)s:%(levelname)s:%(message)s')
else:
	ch.setLevel(logging.INFO)
	formatter = logging.Formatter('%(asctime)s:%(levelname)s:%(message)s')
fh.setFormatter(formatter)
ch.setFormatter(formatter)
logger.addHandler(fh)
logger.addHandler(ch)

logger.info("Beginning snpiphy run...\n")

if args.force == True:
	logger.info("Force option given. Deleting previous snpiphy run...\n")

#Check for dependencies
logger.info("Checking dependencies...\n")
if not find_executable("bwa"):
	logger.error("bwa not found. Please install the bwa read aligner.")
	sys.exit(1)
if not find_executable("parallel"):
	logger.error("parallel not found. Please install GNU parallel.")
	sys.exit(1)
if not find_executable("samtools"):
	logger.error("samtools not found. Please install samtools.")
	sys.exit(1)
if not find_executable("freebayes"):
	logger.error("freebayes not found. Please install the freebayes variant caller.")
	sys.exit(1)
if not find_executable("FastTree"):
	logger.error("FastTree not found. Please install the FastTree phylogenetics pipeline.")
	sys.exit(1)
if not find_executable("raxmlHPC-PTHREADS"):
	logger.error("raxml not found. Please install the raxml phylogenetics pipeline.")
	sys.exit(1)
if not find_executable("snippy"):
	logger.error("snippy not found. Please install snippy.")
	sys.exit(1)
if not find_executable("run_gubbins.py"):
	logger.error("gubbins not found. Please install gubbins.")
	sys.exit(1)

logger.info("All dependencies found.\n")

logger.info("Running snpiphy with the following options:")
if args.reads_dir != False:
	logger.info("Reads directory = {}".format(args.reads_dir))
	reads = (os.path.abspath(args.reads_dir), 'dir')
if args.reads_list != False:
	logger.info("Reads list = {}".format(args.reads_list))
	reads = (os.path.abspath(args.reads_list), 'list')
logger.info("output-directory = {}".format(args.outdir))
logger.info("threads = {}".format(args.threads))
logger.info("coverage cutoff = {}".format(args.cutoff))
logger.info("force overwrite of previous output: {}".format(args.force))
logger.info("prefix = {}".format(args.prefix))
logger.info("use parallel for multiprocessing = {}".format(args.parallel))
logger.info("raxml model = {}".format('GTRGAMMA' if args.gamma_model else 'GTRCAT'))
logger.info("tree builder = {}".format(args.tree_builder))
logger.info("single end data = {}\n".format(args.single_end))
logger.info("recombination filtering deactivated? = {}\n".format(args.no_recombination_filter))


job = SnpiPhy(args.threads, args.cutoff, reads, os.path.abspath(args.outdir), os.path.abspath(args.reference), args.prefix, args.gamma_model, args.tree_builder, args.single_end, args.no_recombination_filter)
job.remove_results_without_reads()
logger.info("Processing {} samples:\n".format(len(job.sample_names)))
logger.debug("Samples to process:\n\n{}\n".format('\n'.join(job.sample_names)))
logger.debug("Samples that need reference alignment:\n\n{}\n".format('\n'.join(job.snippy_not_done)))
logger.info("STEP 1: Aligning sequences to reference genome...\n")
if len(job.snippy_not_done) != 0:
	if args.parallel == True:
		commands = job.gen_snippy_parallel_lists()
		job.run_parallel_snippy(commands)
	else:
		snippy_run_list = job.gen_snippy_run_list()
		for read_path, read_type in snippy_run_list:
			job.run_snippy(read_path, read_type)
logger.info("STEP 2: Generating raw core genome alignment...\n")
job.run_snippy_core()
logger.info("STEP 3: Filtering Ns from genome alignment...\n")
job.Ns2gaps()
logger.info("STEP 4: Building maximum likelihood phylogenetic tree...\n")
job.build_initial_tree()
if args.no_recombination_filter == False:
	logger.info("STEP 5: Predicting and filtering recombination events from genome alignment...\n")
	job.filter_recombinant_positions()
else:
	logger.info("STEP 5: Recombination filtering has been skipped...\n")
if args.tree_builder == 'raxml':
	logger.info("STEP 6: Running bootstrap check...\n")
	job.gen_bootstrap_tree()
	job.gen_final_tree()
	logger.info("STEP 7: Performing pairwise core SNP counts...\n")
else:
	logger.info("STEP 6: Performing pairwise core SNP counts...\n")
job.count_core_snps()
logger.info("Job complete, thanks for using snpiphy.")
